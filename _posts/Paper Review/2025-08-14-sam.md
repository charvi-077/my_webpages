---
title: "Segment Anything - SAM"
last_modified_at: 2025-08-14-SAM
categories:
  - computer-vision
tags:
  - Transformer
  - Segmentation
  - Computer Vision
excerpt: "If Kakao 2022에서 소개된 MFIM 논문 리뷰 (EVCA 2022)"
use_math: true
classes: wide
---

> [[Paper](https://doi.org/10.48550/arXiv.2304.02643)]  
> Good tutorial links : [[Blog](https://maucher.pages.mi.hdm-stuttgart.de/orbook/deeplearning/SAM.html)]
> Meta AI Research, FAIR


## Introduction : 

<center><img src='{{"/assets/img/sam/image.png" | relative_url}}' width="80%"></center>
<center><em>sam-block-diagram</em></center>




iou score , it tries to predict the iou without seeing the labels , so we used iou token 

mask token similarly is place holder for 4 generated classes or masks 

two way attention block is nothing but cross attention from image to prompt and vice versa 

and it is used two times. 















_run_single_frame_inference
we go into _get_image_features - there we need to check what is backbone_out ? 

backbone_out
{'vision_features': tensor([[[[ 0.0105,  0.0166,  0.0436,  ...,  0.0533,  0.0909,  0.1100],
          [ 0...1388,  0.0138]]]],
       device='cuda:0'), 'vision_pos_enc': [tensor([[[[ 2.4536e-02,  2.4536e-02,  2.4536e-02,  ...,  2.4536e-02,
            2.45...]]], device='cuda:0', dtype=torch.float16), tensor([[[[ 4.9072e-02,  4.9072e-02,  4.9072e-02,  ...,  4.9072e-02,
            4.90...]]], device='cuda:0', dtype=torch.float16), tensor([[[[ 9.8017e-02,  9.8017e-02,  9.8017e-02,  ...,  9.8017e-02,
            9.80...000e+00,  1.0000e+00]]]], device='cuda:0')], 'backbone_fpn': [tensor([[[[-2.3926e-01, -1.3269e-01, -1.0803e-01,  ..., -1.4734e-01,
           -1.18...]]], device='cuda:0', dtype=torch.float16), tensor([[[[-5.5481e-02, -1.0376e-02, -6.1035e-04,  ..., -2.8076e-03,
            1.78...]]], device='cuda:0', dtype=torch.float16), tensor([[[[ 0.0105,  0.0166,  0.0436,  ...,  0.0533,  0.0909,  0.1100],
          [ 0...1388,  0.0138]]]],
       device='cuda:0')]}
we get expanded_backbone_out 
_prepare_backbone_features 

1. features = expanded image + features 

2. track_step
_prepare_memory_conditioned_features 

3. _forward_sam_heads
sam_prompt_encoder
promptEncoder ----> forward ! 

  point_embedding
  return sparse and dense embedding 

  sam_mask_decoder
  get dense positional encoding 

  predict_masks (mask_decoder.py)
  run transformers 
  it runs two way transformers 

  then we get the attention layer again and see the output ! 

  









